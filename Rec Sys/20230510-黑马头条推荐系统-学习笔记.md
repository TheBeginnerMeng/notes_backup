黑马头条推荐系统

# 一、黑马头条推荐架构介绍

1、业务

黑马头条推荐系统建立在海量用户和海量文章之上，使用 Lambda 大数据实时和离线计算架构，利用用户在 APP 上的点击、浏览、收藏等行为建立用户与文章之间的画像关系，通过机器学习算法（推荐）进行智能推荐。同时，增加热门文章和新文章的占比，达到千人千面的推荐效果。

2、场景

- 首页频道推荐
- 文章相似推荐

![首页频道推荐示意图](C:\Users\dell\AppData\Roaming\Typora\typora-user-images\image-20230510101808472.png)

3、架构与业务流

![img](file:///D:/tim_files/01-%E5%B7%A5%E4%BD%9C%E7%9B%AE%E5%BD%95/2022%E5%B9%B4/20220822-%E6%8E%A8%E8%8D%90%E7%BD%91%E7%BB%9C%E8%B5%84%E6%BA%90%EF%BC%88%E9%BB%91%E9%A9%AC%E5%B0%9A%E7%A1%85%E8%B0%B7%EF%BC%89/2-%E9%BB%91%E9%A9%AC%E5%A4%B4%E6%9D%A1%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/%E9%BB%91%E9%A9%AC%E6%8E%A8%E8%8D%90%E8%AF%BE%E4%BB%B6%E5%AE%8C%E6%95%B41.0/images/%E9%BB%91%E9%A9%AC%E5%A4%B4%E6%9D%A1%E6%8E%A8%E8%8D%90%E6%9E%B6%E6%9E%84%E5%9B%BE.png)

> 基础数据层

包括业务数据和用户行为日志数据

业务数据：用户数据和文章数据，其中用户数据是用户的基础数据（年龄、性别、地区等），文章数据则是文章的基本信息（标题、正文、发布时间等）。

用户行为日志数据：来源于前端埋点。

> 数据处理层

基础计算：基于离线和实时数据，对各类基础数据计算成用户画像、文章画像

召回与排序：召回阶段通过各种算法从海量文章中筛选用户可能感兴趣的文章候选集合，排序阶段（点击率预估模型）对集合中的文章进行针对用户的模型结果排序，生成一个排序列表。

> 推荐业务层

通过对外提供 RPC 接口来实现推荐业务的接入。

# 二、离线计算更新 Item 画像

## 2.1 数据库迁移

目的：通过 Sqoop 将业务数据导入到 Hive。

### 2.1.1 Sqoop 导入形式

> 全量导入

```shell
#!/bin/bash

array=(user_profile user_basic news_user_channel news_channel user_follow user_blacklist user_search news_collection news_article_basic news_article_content news_read news_article_statistic user_material)

for table_name in ${array[@]};
do
    sqoop import \
        --connect jdbc:mysql://192.168.19.137/toutiao \
        --username root \
        --password password \
        --table $table_name \
        --m 5 \
        --hive-home /root/bigdata/hive \
        --hive-import \
        --create-hive-table  \
        --hive-drop-import-delims \
        --warehouse-dir /user/hive/warehouse/toutiao.db \
        --hive-table toutiao.$table_name
done
```

> 增量导入

- append：通过指定一个递增的列，如：–incremental append –check-column num_iid –last-value 0

- incremental：时间戳，如：

    ```shell
    --incremental lastmodified \
    --check-column column \
    --merge-key key \
    --last-value '2023-05-10 11:20:00'
    # 表示只导入 check-column 的列比 '2023-05-10 11:20:00' 更大的数据，按照 key 合并
    ```

- 导入最终结果有两种形式，选择后者

    - Sqoop 直接导入到 Hive（--incremental lastmodified 模式不支持导入到 Hive）
    - Sqoop 导入到 HDFS，然后建立 Hive 表关联
        - --target-dir /user/hive/warehouse/toutiao.db/

### 2.1.2 Sqoop 迁移案例

避坑指南：导入数据到 Hive 中，需要在创建 Hive 表中加入 `row format delimited fields terminated by ','`。

1、user_profile表

- 使用 lastmodified 模式
- mysql 数据库中更新数据时 update_time 会修改最近时间，按照 user_id 合并
- 指定 last_time 时间

2、user_basic 表

- 使用 lastmodified 模式
- last_login 作为更新时间
- 指定 last_time 时间，按照 user_id 合并

MySQL 导入对应的 Hive 类型：

```
MySQL(bigint) --> Hive(bigint) 
MySQL(tinyint) --> Hive(tinyint) 
MySQL(int) --> Hive(int) 
MySQL(double) --> Hive(double) 
MySQL(bit) --> Hive(boolean) 
MySQL(varchar) --> Hive(string) 
MySQL(decimal) --> Hive(double) 
MySQL(date/timestamp) --> Hive(string)
```

创建 Hive 表：

```sql
create table user_profile(
user_id BIGINT comment "userID",
gender BOOLEAN comment "gender",
birthday STRING comment "birthday",
real_name STRING comment "real_name",
create_time STRING comment "create_time",
update_time STRING comment "update_time",
register_media_time STRING comment "register_media_time",
id_number STRING comment "id_number",
id_card_front STRING comment "id_card_front",
id_card_back STRING comment "id_card_back",
id_card_handheld STRING comment "id_card_handheld",
area STRING comment "area",
company STRING comment "company",
career STRING comment "career")
COMMENT "toutiao user profile"
row format delimited fields terminated by ','
LOCATION '/user/hive/warehouse/toutiao.db/user_profile';

create table user_basic(
user_id BIGINT comment "user_id",
mobile STRING comment "mobile",
password STRING comment "password",
profile_photo STRING comment "profile_photo",
last_login STRING comment "last_login",
is_media BOOLEAN comment "is_media",
article_count BIGINT comment "article_count",
following_count BIGINT comment "following_count",
fans_count BIGINT comment "fans_count",
like_count BIGINT comment "like_count",
read_count BIGINT comment "read_count",
introduction STRING comment "introduction",
certificate STRING comment "certificate",
is_verified BOOLEAN comment "is_verified")
COMMENT "toutiao user basic"
row format delimited fields terminated by ','
LOCATION '/user/hive/warehouse/toutiao.db/user_basic';
```

导入脚本，创建一个脚本文件 import_incremental.sh 并执行：

```shell
time=`date +"%Y-%m-%d" -d "-1day"`
declare -A check
check=([user_profile]=update_time [user_basic]=last_login [news_channel]=update_time)
declare -A merge
merge=([user_profile]=user_id [user_basic]=user_id [news_channel]=channel_id)

for k in ${!check[@]}
do
    sqoop import \
        --connect jdbc:mysql://192.168.19.137/toutiao \
        --username root \
        --password password \
        --table $k \
        --m 4 \
        --target-dir /user/hive/warehouse/toutiao.db/$k \
        --incremental lastmodified \
        --check-column ${check[$k]} \
        --merge-key ${merge[$k]} \
        --last-value ${time}
done
```

3、文章表导入 news_article_basic，news_article_content，news_channel

- news_article_basic：
    - 按照 review_time 更新
    - 合并按照 article_id
    - 指定时间

创建表：

```sql
create table news_article_basic(
article_id BIGINT comment "article_id",
user_id BIGINT comment "user_id",
channel_id BIGINT comment "channel_id",
title STRING comment "title",
status BIGINT comment "status",
update_time STRING comment "update_time")
COMMENT "toutiao news_article_basic"
row format delimited fields terminated by ','
LOCATION '/user/hive/warehouse/toutiao.db/news_article_basic';
```

注意，原 MySQL 数据库中某些字段的值存在一些特定的字符，如“,”、“\t”、“\n”，这些字符导入到 Hadoop 后被 Hive 读取失败，解析时会被认为是另一条数据或者多一个字段。

解决方案：

- 在导入时，加入 `--query`参数，从数据库中选中相应字段，过滤相应内容，使用 REPLACE、CHAR 进行字符替换
- MySQL 表中存在 tinyint 时必须在 connect 中加入：`?tinyInt1isBit=false`

导入脚本：

```shell
# 方案：导入方式，过滤相关字符
sqoop import \
    --connect jdbc:mysql://192.168.19.137/toutiao?tinyInt1isBit=false \
    --username root \
    --password password \
    --m 4 \
    --query 'select article_id, user_id, channel_id, REPLACE(REPLACE(REPLACE(title, CHAR(13),""),CHAR(10),""), ",", " ") title, status, update_time from news_article_basic WHERE $CONDITIONS' \
    --split-by user_id \
    --target-dir /user/hive/warehouse/toutiao.db/news_article_basic \
    --incremental lastmodified \
    --check-column update_time \
    --merge-key article_id \
    --last-value ${time}
```

4、news_channel

- 按照 update_time 更新
- 按照 channel_id 合并
- 指定时间

```sql
create table news_channel(
channel_id BIGINT comment "channel_id",
channel_name STRING comment "channel_name",
create_time STRING comment "create_time",
update_time STRING comment "update_time",
sequence BIGINT comment "sequence",
is_visible BOOLEAN comment "is_visible",
is_default BOOLEAN comment "is_default")
COMMENT "toutiao news_channel"
row format delimited fields terminated by ','
LOCATION '/user/hive/warehouse/toutiao.db/news_channel';
```

5、由于 news_article_content 文章内容表中含有过多特殊字符，选择直接全量导入

```sql
# 全量导入(表只是看结构，不需要在 HIVE 中创建，因为是直接导入 HIVE，会自动创建 news_article_content)
create table news_article_content(
article_id BIGINT comment "article_id",
content STRING comment "content")
COMMENT "toutiao news_article_content"
row format delimited fields terminated by ','
LOCATION '/user/hive/warehouse/toutiao.db/news_article_content';
```

直接导入到 HIVE，导入脚本为：

```shell
sqoop import \
    --connect jdbc:mysql://192.168.19.137/toutiao \
    --username root \
    --password password \
    --table news_article_content \
    --m 4 \
    --hive-home /root/bigdata/hive \
    --hive-import \
    --hive-drop-import-delims \
    --hive-table toutiao.news_article_content \
    --hive-overwrite
```

## 2.2 用户行为收集到 Hive

### 2.2.1 收集用户行为的目的

- 便于了解分析用户的行为、喜好变化
- 为用户建立画像提供依据

### 2.2.2 如何收集用户日志

一般用户有很多日志，当前推荐系统项目中的推荐场景日志统一到用户行为日志中，还有其他的业务场景（如下单日志、支付日志等）

> 埋点参数

定义：在应用中特定的流程收集一些信息，用来跟踪应用使用的情况，用来进一步优化产品或是提供运营的数据支撑。

重要性：埋点数据是推荐系统的基石，模型训练和效果数据统计都基于埋点数据，需要保证埋点数据的准确性。

### 2.2.3 埋点需求整理

用户对文章的行为特点：点击、浏览、收藏、分享等行为，基于这些行为制定埋点需求：

- 埋点场景
    - 首页中的各个频道
- 埋点事件号
    - 停留时间
    - 点击事件
    - 曝光事件（相当于刷新一次请求推荐新文章）
    - 收藏事件
    - 分享事件

- 埋点参数的文件结构

```json
# 曝光的参数，
{"actionTime":"2019-04-10 18:15:35","readTime":"","channelId":0,"param":{"action": "exposure", "userId": "2", "articleId": "[18577, 14299]", "algorithmCombine": "C2"}}

# 对文章发生行为的参数
{"actionTime":"2019-04-10 18:12:11","readTime":"2886","channelId":18,"param":{"action": "read", "userId": "2", "articleId": "18005", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:32","readTime":"","channelId":18,"param":{"action": "click", "userId": "2", "articleId": "18005", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:34","readTime":"1053","channelId":18,"param":{"action": "read", "userId": "2", "articleId": "18005", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:36","readTime":"","channelId":18,"param":{"action": "click", "userId": "2", "articleId": "18577", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:38","readTime":"1621","channelId":18,"param":{"action": "read", "userId": "2", "articleId": "18577", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:39","readTime":"","channelId":18,"param":{"action": "click", "userId": "1", "articleId": "14299", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:39","readTime":"","channelId":18,"param":{"action": "click", "userId": "2", "articleId": "14299", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:41","readTime":"914","channelId":18,"param":{"action": "read", "userId": "2", "articleId": "14299", "algorithmCombine": "C2"}}
{"actionTime":"2019-04-10 18:15:47","readTime":"7256","channelId":18,"param":{"action": "read", "userId": "1", "articleId": "14299", "algorithmCombine": "C2"}}
```

### 2.2.4 离线部分-用户日志收集

目的：**通过 Flume 将业务数据服务器 A 的日志收集到 Hadoop 服务器 HDFS 的 Hive 中**。

> 步骤

- 创建 Hive 对应的日志收集表
- Flume 收集日志的配置
- 开启收集命令

1、Flume 读取设置

进入`flume/conf`目录，创建一个`collect_click.conf`文件，写入 flume 的配置：

- sources：为实时查看文件末尾，interceptors 解析 json 文件
- channels：指定内存存储，并且制定 batchData 的大小，PutList 和 TakeList 的大小见参数，Channel 总容量大小见参数
- 指定 sink：形式直接到 hdfs，以及路径，文件大小策略默认 1024、event 数量策略、文件闲置时间

```conf
a1.sources = s1
a1.sinks = k1
a1.channels = c1

a1.sources.s1.channels= c1
a1.sources.s1.type = exec
a1.sources.s1.command = tail -F /root/logs/userClick.log
a1.sources.s1.interceptors=i1 i2
a1.sources.s1.interceptors.i1.type=regex_filter
a1.sources.s1.interceptors.i1.regex=\\{.*\\}
a1.sources.s1.interceptors.i2.type=timestamp

# channel1
a1.channels.c1.type=memory
a1.channels.c1.capacity=30000
a1.channels.c1.transactionCapacity=1000

# k1
a1.sinks.k1.type=hdfs
a1.sinks.k1.channel=c1
a1.sinks.k1.hdfs.path=hdfs://192.168.19.137:9000/user/hive/warehouse/profile.db/user_action/%Y-%m-%d
a1.sinks.k1.hdfs.useLocalTimeStamp = true
a1.sinks.k1.hdfs.fileType=DataStream
a1.sinks.k1.hdfs.writeFormat=Text
a1.sinks.k1.hdfs.rollInterval=0
a1.sinks.k1.hdfs.rollSize=10240
a1.sinks.k1.hdfs.rollCount=0
a1.sinks.k1.hdfs.idleTimeout=60
```

在 profile 数据库中创建 user_action 表，指定格式

```mysql
create table user_action(
actionTime STRING comment "user actions time",
readTime STRING comment "user reading time",
channelId INT comment "article channel id",
param map comment "action parameter")
COMMENT "user primitive action"
PARTITIONED BY(dt STRING)
ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'
LOCATION '/user/hive/warehouse/profile.db/user_action';
```

ROW FORMAT SERDE 'org.apache.hive.hcatalog.data.JsonSerDe'：添加一个 json 格式匹配

开启日志收集的命令：

```
/root/bigdata/flume/bin/flume-ng agent -c /root/bigdata/flume/conf -f /root/bigdata/flume/conf/collect_click.conf -Dflume.root.logger=INFO,console -name a1
```

启动监听 Flume 收集日志程序，将启动 flume 的程序建立成 collect_click.sh 脚本，flume 启动需要相关 Hadoop、Java 环境，可在 shell 脚本中添加。

```shell
#!/usr/bin/env bash

export JAVA_HOME=/root/bigdata/jdk
export HADOOP_HOME=/root/bigdata/hadoop
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin

/root/bigdata/flume/bin/flume-ng agent -c /root/bigdata/flume/conf -f /root/bigdata/flume/conf/collect_click.conf -Dflume.root.logger=INFO,console -name a1
```

使用 supervisor 来管理 flume 日志采集进程，在 supervisor 的 rec.conf 中添加：

```
[program:collect-click]
command=/bin/bash /root/toutiao_project/scripts/collect_click.sh
user=root
autorestart=true
redirect_stderr=true
stdout_logfile=/root/logs/collect.log
loglevel=info
stopsignal=KILL
stopasgroup=true
killasgroup=true
```

## 2.3 Supervisor 进程管理

Supervisor 作为进程管理工具，可以很方便的监听、启动、停止、重启一个或多个进程。用 Supervisor 管理的进程，当一个进程被意外杀死，Supervisor 监听到进程被杀死后，会自动将其重新拉起，实现进程自动恢复的功能，不需要自己通过 Shell 脚本来实现。

> 使用

1、配置 Supervisor 生成配置文件（`echo_supervisord_conf > supervisord.conf`）

2、配置进程的` reco.conf`

```
[program:recogrpc]
command=/root/anaconda3/envs/reco_sys/bin/python /root/headlines_project/recommend_system/ABTest/routing.py
directory=/root/headlines_project/recommend_system/ABTest
user=root
autorestart=true
redirect_stderr=true
stdout_logfile=/root/logs/reco.log
loglevel=info
stopsignal=KILL
stopasgroup=true
killasgroup=true

[program:kafka]
command=/bin/bash /root/headlines_project/scripts/startKafka.sh
directory=/root/headlines_project/scripts
user=root
autorestart=true
redirect_stderr=true
stdout_logfile=/root/logs/kafka.log
loglevel=info
stopsignal=KILL
stopasgroup=true
killasgroup=true
```

3、修改 Supervisor 配置文件

vim 编辑 supervisord.conf，修改

```
[include]
files = relative/directory/*.ini # 修改为 files = /etc/supervisor/*.conf
```

4、开启 Supervisor，启动 `supervisord -c /etc/supervisord.conf`

5、Supervisor 管理进程

可以用 supervisorctl 来管理 Supervisor

```
supervisorctl

> status    # 查看程序状态
> start apscheduler  # 启动 apscheduler 单一程序
> stop toutiao:*   # 关闭 toutiao组 程序
> start toutiao:*  # 启动 toutiao组 程序
> restart toutiao:*    # 重启 toutiao组 程序
> update    ＃ 重启配置文件修改过的程序
```

## 2.4 离线画像业务

